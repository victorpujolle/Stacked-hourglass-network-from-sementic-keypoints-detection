hg_test_13_43 : result are slightly better with the domain layer only on out[nStack - 1] than on tf.stack(out)
hg_test_14 : addition of a dense layer of 512 neurones in the gradient classifier : does not work even on the blender data
hg_test_15 : rms optimizer -> gradient descent optimizer : not working att all
hg_test_16_41 : rms optimizer : perform well on blender, not at all on real data
hg_test_17_40 : add a sigmoid function at the end of the domain classifier : perform well on blender, not at all on real data
hg_test_18_40 : add a dense layer but keeping the sigmoid : perform well on blender, not at all on real data
hg_test_19_200 : re implementing the gamma : this is not better

Now the evaluation will be one images where the book is masked
There is a high angular error with the evaluation due probably to the fact that 00_1_4_7 and 01_1_4_7 present the same box but with a rotation a 180 degrees


hg_test_23_59 : error during the training, stopped at 59 epochs. The network performs well. This network use the gamma to modify the loss function. This does not seems to show any improvement.
hg_test_24_41 : suppression of the domain loss to evaluate the impact of the domain adaptation on the result, the impact is not observable
hg_test_25 : modification of the domain classifier => this is not a significant improvement
hg_test_26 : domain classifier : add a dense layer -> the network does not perform good
hg_test_27 : suppression of the gamma, addition of a dropout layer in the domain classifier to reduce the over fitting : not bad
hg_test_28 : learning rate : 0.001 -> 0.00025 and dropout rate : 0.2 -> 0.4 : I try to see if over fitting is a issue