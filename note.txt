hg_test_13_43 : result are slightly better with the domain layer only on out[nStack - 1] than on tf.stack(out)
hg_test_14 : addition of a dense layer of 512 neurones in the gradient classifier : does not work even on the blender data
hg_test_15 : rms optimizer -> gradient descent optimizer : not working att all
hg_test_16_41 : rms optimizer : perform well on blender, not at all on real data
hg_test_17_40 : add a sigmoid function at the end of the domain classifier : perform well on blender, not at all on real data
hg_test_18_40 : add a dense layer but keeping the sigmoid : perform well on blender, not at all on real data
hg_test_19_200 : re implementing the gamma : this is not better

Now the evaluation will be one images where the book is masked
There is a high angular error with the evaluation due probably to the fact that 00_1_4_7 and 01_1_4_7 present the same box but with a rotation a 180 degrees


hg_test_23_59 : error during the training, stopped at 59 epochs. The network performs well. This network use the gamma to modify the loss function. This does not seems to show any improvement.
hg_test_24_41 : suppression of the domain loss to evaluate the impact of the domain adaptation on the result, the impact is not observable
hg_test_25 : modification of the domain classifier => this is not a significant improvement
hg_test_26 : domain classifier : add a dense layer -> the network does not perform good
hg_test_27 : suppression of the gamma, addition of a dropout layer in the domain classifier to reduce the over fitting : not bad
hg_test_27 : learning rate : 0.001 -> 0.00025 and dropout rate : 0.2 -> 0.4 : I try to see if over fitting is a issue -> the net does not performs as well as before, one can conclude that overfitting is not an issue know
hg_test_28 : suppression of the dropout layer in the domain classifier : the net has good results but could be improve
hg_test_29 : augmentation of the domain classifier capacity and reduction of the dropout rate to see if under-fitting is an issue : improvement
hg_test_30 : is the gamma useful ? -> the suppression of gamma lead to a small improvement -> gamma is not useful
hg_test_31 : reduction of the dropout rate 0.2 -> 0.1 : the performance decreased -> optimal dropout rate between 0.3 and 0.1
hg_test_32 : test for all the dropout rate between 0.1 and 0.3
hg_test_34 : it could be interesting to see if the metric of the heatmap loss is good, A L2 mesure could be more effective. Moreover, a hyperparameter to weight the importance of the two losses could be useful.
hg_test_35_200 : the L2 norm shows good
hg_test_36 : DANN is disable to see is there is a difference
hg_test_36 : same as the precedent but the DANN is enable to mesure the difference (2 layers)
hg_test_37 : try to see what is the best size for the domain classifier : here 1 layer
hg_test_37 : 3 layers, really small improvement of the results
hg_test_38 : 3 layers, relu activation function to add some non linearity : the net does not perform as well as before
hg_test_38 : 2 layers, relu activation : the results are worse than before
hg_test_39 : 3 layers, relu, dropout : not so bad but less efficient that the 3 layers without relu
hg_test_39 : 3 layers, dropout, no activation : the network overcome the others
hg_test_40 : training one the data_all_plus where there is image without the box : performs good but extremelly unstable -> the training set was not well configurated
hg_test_40 : try to see if cross entropy is a better loss function
hg_test_40 : tf.nn.sigmoid_cross_entropy -> tf.nn.softmax_cross_entropy : nan everywhere so evaluation is impossible but it works on the video pretty well and it is stable
hg_test_41 : tf.nn.sigmoid cross entropy : the training set all_plus is not good because if the label are 0 the net is trained to predict every key points at the POSITION 0
hg_test_42 : try with softmax_cross_entropy one a correct dataset (train_all) : the net diverged

hg_test_42 : trained on train_all _plus : does work well, this training set reduce drastically the accuracy. The network is able the evoid key point on the nutella box but it is now enable to find all the key points of the cheezit with the precedente accuracy.

on train_all, based on the accuracy mesurement of val_new, the domain classifier with this option seems to perform well self._dense_drop(inputs=dropout1,units=1024, activation=None, dropout_rate=0.4, n=3)

hg_test_43 : we will compare the to mesure (cross entropy and l2)
hg_test_43 : dr = 0.25, dr_domain = 0.4, lr = 0.001 : results are really good, but not on the video
hg_test_44 : activation=relu, n=4
hg_test_45 : activation=relu, n=3
hg_test_46 : activation=None, n=4
hg_test_47 : non dann to mesure difference